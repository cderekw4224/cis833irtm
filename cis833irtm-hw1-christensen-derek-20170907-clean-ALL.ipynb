{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2017fiedler](http://www.engg.ksu.edu/images/2017fiedler.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-State Honor Code\n",
    ">### \"On my honor, as a student, I have neither given nor received unauthorized aid on this academic work.\"\n",
    ">### \"The assignment I am submitting contains my own words without borrowing the words of other people from the Internet or other sources (e.g., articles, lecture notes).”\n",
    ">### Derek W. Christensen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS 833 Information Retrieval and Text Mining\n",
    ">### Fall 2017\n",
    ">### HW 1\n",
    ">### Test Collection, Processing and Analyzing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CiteSeer UMD collection is a standard text document collection, consisting of abstracts of research articles from Computer Science, which are sampled from the CiteSeer digital library.\n",
    "#### The dataset is available for download from Online K-State.\n",
    "#### **Tasks:**\n",
    "1. Write a program that preprocesses the collection.<br>\n",
    "This preprocessing stage should specifically include a function that tokenizes the text.<br>\n",
    "In doing so, tokenize on whitespace and remove punctuation.<br>\n",
    "2. Determine the frequency of occurrence for all the words in the collection. Answer the following questions:<br>\n",
    "a. What is the total number of words in the collection?<br>\n",
    "b. What is the vocabulary size? (i.e., number of unique terms).<br>\n",
    "c. What are the top 20 words in the ranking? (i.e., the words with the highest frequencies).<br>\n",
    "d. From these top 20 words, which ones are stop-words?<br>\n",
    "e. What is the minimum number of unique words accounting for 15% of the total number\n",
    "of words in the collection?<br>\n",
    "<br>\n",
    "Example: if the total number of words in the collection is 100, and we have the fol-\n",
    "lowing word-frequency pairs:<br>\n",
    "<br>\n",
    "word tf<br>\n",
    "the 20<br>\n",
    "of 10<br>\n",
    "a 10<br>\n",
    "data 8<br>\n",
    "mining 7<br>\n",
    "<br>\n",
    "the answer to this question will be (1 word accounts for 15% of the total 100 words).<br>\n",
    "\n",
    "3. Integrate the Porter stemmer and a stopword eliminator into your code.<br>\n",
    "Answer again questions a.-e. from the previous point.<br>\n",
    "(See below a link to a Java Porter stemmer implementation and to a stopwords list).<br>\n",
    "https://www.dropbox.com/s/rexuzz3j56vi4bt/Porter.java <br>\n",
    "https://www.dropbox.com/s/5789sj8v07j2id0/stopwords.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Write a program that preprocesses the collection.<br>\n",
    "This preprocessing stage should specifically include a function that tokenizes the text.<br>\n",
    "In doing so, tokenize on whitespace and remove punctuation.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import nltk, which is a python package for natural language processing\n",
    "import nltk\n",
    "# to remove stropwords\n",
    "from nltk.corpus import stopwords\n",
    "# FreqDist, word_tokenize\n",
    "from nltk import FreqDist, sent_tokenize, word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# to read and/or save text files or csv files, import csv\n",
    "import csv\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "# python package for text classification\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#initialize countvectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#initialize TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# support vector machine (another algorithm for classification)\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Evaluating model performance\n",
    "from sklearn import metrics\n",
    "\n",
    "# Excel-like format\n",
    "import pandas as pd\n",
    "# to diplay max rows\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "# to diplay max cols\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "# to define width of cells\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "# package for numbers\n",
    "import numpy as np\n",
    "\n",
    "# Regular Expression\n",
    "import re\n",
    "\n",
    "# WordCloud\n",
    "from os import path\n",
    "#####from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Count words in list\n",
    "from collections import Counter\n",
    "\n",
    "# Pattern\n",
    "#####from pattern.en import sentiment\n",
    "\n",
    "# Seaborn\n",
    "import seaborn\n",
    "\n",
    "# IPyton Display\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# You can include Youtube video in Ipython notebook\n",
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "# Include images in ipython notebook\n",
    "from IPython.display import Image\n",
    "\n",
    "# Include webpages in ipython notebook\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "\n",
    "from operator import itemgetter, attrgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data as list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "review = []\n",
    "\n",
    "##NB: ##--double hash-tags & dashes denote comments in my original code\n",
    "##  : # single hash-tags denote my original code\n",
    "\n",
    "##---------------------------------\n",
    "##-- My original file handling code to read in one file\n",
    "\n",
    "# openfile = open('data/abdennadher00experimental', 'rb')\n",
    "\n",
    "##--openfile = open('data/*.txt', 'rb')\n",
    "\n",
    "# r = csv.reader(openfile)\n",
    "# for i in r:\n",
    "     ##-- get the first column only (ignoring the second column)\n",
    "#    review.append(i)    \n",
    "# openfile.close()\n",
    "\n",
    "############################################################\n",
    "# path to citeseer directory\n",
    "#dir_path = ''./citeseer\"\n",
    "\n",
    "#dir_path = ''./data\"  # didn't work\n",
    "#dir_path = ''/data\"  # didn't work\n",
    "#dir_path = ''C:/Users/Derek Christensen/Dropbox/_cis833irtm/hw1/data\"    # didn't work\n",
    "#dir_path = ''./Users/Derek Christensen/Dropbox/_cis833irtm/hw1/data\"    # didn't work\n",
    "#dir_path = ''./Users/Derek Christensen/Dropbox/_cis833irtm/hw1/data\"   # didn't work\n",
    "\n",
    "dir_path = 'C:/Users/Derek Christensen/Dropbox/_cis833irtm/hw1/citeseer/citeseer'\n",
    "\n",
    "# get all files inside the directory\n",
    "files = os.listdir(dir_path)\n",
    "    \n",
    "# tokenize the words based on white space, removes the punctuation\n",
    "for f in files:\n",
    "    ipfile = open(dir_path+'/'+os.path.basename(f),'r')\n",
    "    for i in ipfile:\n",
    "        review.append(i)\n",
    "\n",
    "    # start processing the ipfile\n",
    "#############################################################\n",
    "\n",
    "#print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Semantics for an Agent Communication Language . We address the issue of semantics for an agent communication language. In particular, the semantics of Knowledge Query Manipulation Language (KQML) is investigated. KQML is a language and protocol to support communication between (intelligent) software agents. Based on ideas from speech act theory, we present a semantic description for KQML that associates \"cognitive\" states of the agent with the use of the language\\'s primitives (performatives). We have used this approach to describe the semantics for the whole set of reserved KQML performatives. Our research offers a method for a speech act theory-based semantic description of a language of communication acts. Languages of communication acts address the issue of communication between software applications at a level of abstraction that could prove particularly useful to the emerging software agents paradigm of software design and development. 1 Introduction This research is concerned with communication between software agents [13]...\\n', 'Error-Driven Pruning of Treebank Grammars for Base Noun Phrase Identification Finding simple, non-recursive, base noun phrases is an important subtask for many natural language processing applications. While previous empirical methods for base NP identification have been rather complex, this paper instead proposes a very simple algorithm that is tailored to the relative simplicity of the task. In particular, we present a corpus-based approach for finding base NPs by matching part-ofspeech tag sequences. The training phase of the algorithm is based on two successful techniques: first the base NP grammar is read from a \"treebank\" corpus; then the grammar is improved by selecting rules with high \"benefit\" scores. Using this simple algorithm with a naive heuristic for matching rules, we achieve surprising accuracy in an evaluation on the Penn Treebank Wall Street Journal.  1 Introduction  Finding base noun phrases is a sensible first step for many natural language processing (NLP) tasks: Accurate identification of base noun phrases is arguably the most critical comp...\\n']\n"
     ]
    }
   ],
   "source": [
    "# print 1st 20 words in list review\n",
    "print(review[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# currently the data are in list ... convert to string. \n",
    "# even though there are many files, we will treat it as a single large document\n",
    "tokens = str(review)\n",
    "\n",
    "# You can uncomment the following line if you would like to see to the output\n",
    "\n",
    "#print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Semantics for an Agent Communication Language . We address the issue of semantics for an agent com\n"
     ]
    }
   ],
   "source": [
    "# print 1st 20 characters in list tokens\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lowecases for content analytics ... we assume, for example, LOVE is sames love \n",
    "tokens = tokens.lower()\n",
    "\n",
    "# You can uncomment the following line if you would like to see to the output\n",
    "\n",
    "#print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['semantics for an agent communication language . we address the issue of semantics for an agent com\n"
     ]
    }
   ],
   "source": [
    "# print 1st 20 characters in list tokens\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the dataset contains useless characters and numbers\n",
    "# Remove useless numbers and alphanumerical words\n",
    "# use regular expression ... a-zA-Z0-9 refers to all English characters (lowercase & uppercase) and numbers\n",
    "# ^a-zA-Z0-9 is opposite of a-zA-Z0-9\n",
    "tokens = re.sub(\"[^a-zA-Z0-9]\", \" \", tokens)\n",
    "\n",
    "#print(tokens)\n",
    "\n",
    "# You can uncomment the following line if you would like to see to the output\n",
    "\n",
    "#tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  semantics for an agent communication language   we address the issue of semantics for an agent com\n"
     ]
    }
   ],
   "source": [
    "# print 1st 20 characters in list tokens\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tokenization or word split\n",
    "tokens = word_tokenize(tokens)\n",
    "\n",
    "# You can uncomment the following line if you would like to see to the output\n",
    "\n",
    "#print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['semantics', 'for', 'an', 'agent', 'communication', 'language', 'we', 'address', 'the', 'issue', 'of', 'semantics', 'for', 'an', 'agent', 'communication', 'language', 'in', 'particular', 'the']\n"
     ]
    }
   ],
   "source": [
    "# print 1st 20 tokens in list tokens\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Determine the frequency of occurrence for all the words in the collection. Answer the following questions:<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. What is the total number of words in the collection?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491954\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491954\n"
     ]
    }
   ],
   "source": [
    "totwords = len(tokens)\n",
    "print(totwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#compute frequency distribution for all the tokens in the text\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "\n",
    "# You can uncomment the following line if you would like to see to the output\n",
    "\n",
    "#fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. What is the vocabulary size? (i.e., number of unique terms).<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17325\n"
     ]
    }
   ],
   "source": [
    "print(len(fdist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. What are the top 20 words in the ranking? (i.e., the words with the highest frequencies).<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 25724), ('of', 18713), ('and', 14199), ('a', 13418), ('to', 11644), ('in', 10104), ('for', 7383), ('is', 6579), ('we', 5150), ('that', 4821), ('this', 4448), ('on', 3783), ('are', 3739), ('n', 3329), ('an', 3284), ('with', 3200), ('as', 3078), ('by', 2789), ('data', 2763), ('based', 2534)]\n"
     ]
    }
   ],
   "source": [
    "print(fdist.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. From these top 20 words, which ones are stop-words?<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words in the Top 20:\n",
    "- the\n",
    "- of\n",
    "- and\n",
    "- a\n",
    "- to\n",
    "- in\n",
    "- for\n",
    "- is\n",
    "- we\n",
    "- that\n",
    "- this\n",
    "- on\n",
    "- are\n",
    "- n\n",
    "- an\n",
    "- with\n",
    "- as\n",
    "- by\n",
    "\n",
    "## All of the Top 20 except the last two: data and based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e. What is the minimum number of unique words accounting for 15% of the total number of words in the collection?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73793.1\n"
     ]
    }
   ],
   "source": [
    "totwords15 = totwords * 0.15\n",
    "print(totwords15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13514</th>\n",
       "      <td>the</td>\n",
       "      <td>25724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>of</td>\n",
       "      <td>18713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>and</td>\n",
       "      <td>14199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6757</th>\n",
       "      <td>a</td>\n",
       "      <td>13418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2190</th>\n",
       "      <td>to</td>\n",
       "      <td>11644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9622</th>\n",
       "      <td>zum</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9607</th>\n",
       "      <td>zur</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>zy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7670</th>\n",
       "      <td>zz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14783</th>\n",
       "      <td>zzw8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17325 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0      1\n",
       "13514   the  25724\n",
       "15037    of  18713\n",
       "669     and  14199\n",
       "6757      a  13418\n",
       "2190     to  11644\n",
       "...     ...    ...\n",
       "9622    zum      1\n",
       "9607    zur      1\n",
       "779      zy      1\n",
       "7670     zz      1\n",
       "14783  zzw8      1\n",
       "\n",
       "[17325 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare the results of word frequency on corpus data as a list\n",
    "\n",
    "#freq_word_auto = []\n",
    "freq_word = []\n",
    "\n",
    "# two values or columns in fdist_a\n",
    "for k,v in fdist.items():\n",
    "    freq_word.append([k,v])\n",
    "\n",
    "#make it like an Excel worksheet\n",
    "wordlist = pd.DataFrame(freq_word)\n",
    "\n",
    "#pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "wordlist.sort_values(by=[1,0], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   0   1\n",
      "0       homomorphism   3\n",
      "1             ka1985   1\n",
      "2           testbeds   4\n",
      "3          deferment   1\n",
      "4      telepathology   2\n",
      "...              ...  ..\n",
      "17320        approac   1\n",
      "17321        volumes  13\n",
      "17322     richardson   1\n",
      "17323         invari   1\n",
      "17324        expands   2\n",
      "\n",
      "[17325 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(wordlist.loc[:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "sortwordlist = wordlist.sort_values(by=[1,0], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0      1\n",
      "13514   the  25724\n",
      "15037    of  18713\n",
      "669     and  14199\n",
      "6757      a  13418\n",
      "2190     to  11644\n",
      "...     ...    ...\n",
      "9622    zum      1\n",
      "9607    zur      1\n",
      "779      zy      1\n",
      "7670     zz      1\n",
      "14783  zzw8      1\n",
      "\n",
      "[17325 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(sortwordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   0   1\n",
      "0       homomorphism   3\n",
      "1             ka1985   1\n",
      "2           testbeds   4\n",
      "3          deferment   1\n",
      "4      telepathology   2\n",
      "...              ...  ..\n",
      "17320        approac   1\n",
      "17321        volumes  13\n",
      "17322     richardson   1\n",
      "17323         invari   1\n",
      "17324        expands   2\n",
      "\n",
      "[17325 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(wordlist[:][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0      1\n",
      "0       the  25724\n",
      "1        of  18713\n",
      "2       and  14199\n",
      "3         a  13418\n",
      "4        to  11644\n",
      "...     ...    ...\n",
      "17320   zum      1\n",
      "17321   zur      1\n",
      "17322    zy      1\n",
      "17323    zz      1\n",
      "17324  zzw8      1\n",
      "\n",
      "[17325 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "sortwordlist = sortwordlist.reset_index(drop=True)\n",
    "print(sortwordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0      1\n",
      "0  the  25724\n",
      "1   of  18713\n",
      "2  and  14199\n",
      "3    a  13418\n",
      "4   to  11644\n"
     ]
    }
   ],
   "source": [
    "print(sortwordlist[:5][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words =  491954\n",
      "Min number of unique words to account for 15% =  73793.1\n",
      "\n",
      "25724\n",
      "the\n",
      "18713\n",
      "of\n",
      "14199\n",
      "and\n",
      "13418\n",
      "a\n",
      "11644\n",
      "to\n",
      "\n",
      "83698\n",
      "5\n",
      "\n",
      "Difference between Sum and the last word qty = 83698 - 11644 = 72054\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "i = 0\n",
    "lastwordqty = 0\n",
    "\n",
    "print('Total number of words = ',totwords)\n",
    "print('Min number of unique words to account for 15% = ',totwords15)\n",
    "print()\n",
    "\n",
    "while sum < totwords15:\n",
    "    sum += sortwordlist.loc[i][1]\n",
    "    print(sortwordlist.loc[i][1])\n",
    "    print(sortwordlist.loc[i][0])\n",
    "    lastwordqty = sortwordlist.loc[i][1]\n",
    "    i += 1\n",
    "\n",
    "print()\n",
    "print(sum)\n",
    "print(i)\n",
    "print()\n",
    "\n",
    "diff = sum - lastwordqty\n",
    "print('Difference between Sum and the last word qty =',sum,'-',lastwordqty,'=',diff)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 25724\n",
      "of 18713\n",
      "and 14199\n",
      "a 13418\n",
      "to 11644\n",
      "83698\n",
      "5\n",
      "The minimum number of unique words to account for 15% fo the total number of words in the collection is 5 .\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "i = 0\n",
    "\n",
    "while sum < totwords15:\n",
    "    sum += sortwordlist.loc[i][1]\n",
    "    print(sortwordlist.loc[i][0], sortwordlist.loc[i][1])\n",
    "    i += 1\n",
    "\n",
    "print(sum)\n",
    "print(i)\n",
    "print('The minimum number of unique words to account for 15% fo the total number of words in the collection is', i,'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Integrate the Porter stemmer and a stopword eliminator into your code.\n",
    "Answer again questions a.-e. from the previous point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Stemming/Lemmatization, Stopwords and Shortwords"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "#get stemming words or lemmas\n",
    "#wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stemlemstop_tokens = (wordnet_lemmatizer.lemmatize(word) for word in tokens)\n",
    "stemlemstop_tokens\n",
    "\n",
    "# remove common words\n",
    "stoplist = stopwords.words('english')\n",
    "# if you want to remove additional words\n",
    "#more = set(['much','even','time','story','character','from','went','saw','movie','last','night','see','knew','films','film','one','one','would','also','seen','watch','dvd','get','bit','movies','two','three','whose'])\n",
    "#more = set(['the'])\n",
    "#stoplist = set(stoplist) | more\n",
    "stoplist = set(stoplist)\n",
    "\n",
    "stemlemstop_tokens = [[word for word in text if word not in stoplist] for text in tokens]\n",
    "\n",
    "# remove short words\n",
    "stemlemstop_tokens = [[word for word in tokens if len(word) >= 3 ] for tokens in stemlemstop_tokens]\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['semantics', 'for', 'an', 'agent', 'communication', 'language', 'we', 'address', 'the', 'issue', 'of', 'semantics', 'for', 'an', 'agent', 'communication', 'language', 'in', 'particular', 'the']\n"
     ]
    }
   ],
   "source": [
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0      1\n",
      "0     the  25724\n",
      "1      of  18713\n",
      "2     and  14199\n",
      "3       a  13418\n",
      "4      to  11644\n",
      "..    ...    ...\n",
      "15   with   3200\n",
      "16     as   3078\n",
      "17     by   2789\n",
      "18   data   2763\n",
      "19  based   2534\n",
      "\n",
      "[20 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(sortwordlist[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "491954"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17325"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sortwordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'semant', 'for', 'an', 'agent', u'commun', u'languag', 'we', u'address', 'the', u'issu', 'of', u'semant', 'for', 'an', 'agent', u'commun', u'languag', 'in', 'particular', 'the']\n"
     ]
    }
   ],
   "source": [
    "#stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "print(stemmed_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'semant', 'agent', u'commun', u'languag', u'address', u'issu', u'semant', 'agent', u'commun', u'languag', 'particular', u'semant', u'knowledg', u'queri', u'manipul', u'languag', 'kqml', u'investig', 'kqml', u'languag']\n"
     ]
    }
   ],
   "source": [
    "# remove common words\n",
    "stoplist = stopwords.words('english')\n",
    "# if you want to remove additional words\n",
    "#more = set(['much','even','time','story','character','from','went','saw','movie','last','night','see','knew','films','film','one','one','would','also','seen','watch','dvd','get','bit','movies','two','three','whose'])\n",
    "#more = set(['the'])\n",
    "#stoplist = set(stoplist) | more\n",
    "stoplist = set(stoplist)\n",
    "stemmed_stop_tokens = [word for word in stemmed_tokens if word not in stoplist]\n",
    "print(stemmed_stop_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314896\n"
     ]
    }
   ],
   "source": [
    "print(len(stemmed_stop_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309032\n",
      "[u'semant', 'agent', u'commun', u'languag', u'address', u'issu', u'semant', 'agent', u'commun', u'languag', 'particular', u'semant', u'knowledg', u'queri', u'manipul', u'languag', 'kqml', u'investig', 'kqml', u'languag']\n"
     ]
    }
   ],
   "source": [
    "# Filter non-alphanumeric characters from tokens\n",
    "stemmed_stop_tokens = [word for word in stemmed_stop_tokens if word.isalpha()]\n",
    "print(len(stemmed_stop_tokens))\n",
    "print(stemmed_stop_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299345\n",
      "[u'semant', 'agent', u'commun', u'languag', u'address', u'issu', u'semant', 'agent', u'commun', u'languag', 'particular', u'semant', u'knowledg', u'queri', u'manipul', u'languag', 'kqml', u'investig', 'kqml', u'languag']\n"
     ]
    }
   ],
   "source": [
    "#remove short words\n",
    "stemmed_stop_tokens = [word for word in stemmed_stop_tokens if len(word) >= 3]\n",
    "print(len(stemmed_stop_tokens))\n",
    "print(stemmed_stop_tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. What is the total number of words in the collection?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299345\n"
     ]
    }
   ],
   "source": [
    "print(len(stemmed_stop_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fdist_stemmed_stop_tokens = nltk.FreqDist(stemmed_stop_tokens)\n",
    "\n",
    "# You can uncomment the following line if you would like to see to the output\n",
    "\n",
    "#fdist_stemmed_stop_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. What is the vocabulary size? (i.e., number of unique terms).<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10270\n"
     ]
    }
   ],
   "source": [
    "print(len(fdist_stemmed_stop_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. What are the top 20 words in the ranking? (i.e., the words with the highest frequencies).<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'thi', 4448),\n",
       " (u'system', 3766),\n",
       " ('use', 3748),\n",
       " ('agent', 3258),\n",
       " ('data', 2763),\n",
       " (u'base', 2763),\n",
       " (u'inform', 2437),\n",
       " ('model', 2349),\n",
       " ('paper', 2249),\n",
       " (u'queri', 1944),\n",
       " ('user', 1884),\n",
       " (u'learn', 1820),\n",
       " ('algorithm', 1597),\n",
       " (u'problem', 1575),\n",
       " ('web', 1560),\n",
       " (u'comput', 1554),\n",
       " (u'applic', 1551),\n",
       " ('approach', 1546),\n",
       " ('present', 1508),\n",
       " (u'databas', 1442)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_stemmed_stop_tokens.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. From these top 20 words, which ones are stop-words?<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words in the Top 20:\n",
    "- thi (this)\n",
    "- use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e. What is the minimum number of unique words accounting for 15% of the total number of words in the collection?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299345\n"
     ]
    }
   ],
   "source": [
    "totwords_stem = len(stemmed_stop_tokens)\n",
    "print(totwords_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44901.75\n"
     ]
    }
   ],
   "source": [
    "totwords15_stem = totwords_stem * 0.15\n",
    "print(totwords15_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3134</th>\n",
       "      <td>thi</td>\n",
       "      <td>4448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290</th>\n",
       "      <td>system</td>\n",
       "      <td>3766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6978</th>\n",
       "      <td>use</td>\n",
       "      <td>3748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5904</th>\n",
       "      <td>agent</td>\n",
       "      <td>3258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4120</th>\n",
       "      <td>base</td>\n",
       "      <td>2763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10168</th>\n",
       "      <td>zkdw</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3446</th>\n",
       "      <td>zlwk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2885</th>\n",
       "      <td>zrunlqj</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5689</th>\n",
       "      <td>zum</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3383</th>\n",
       "      <td>zur</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10270 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0     1\n",
       "3134       thi  4448\n",
       "2290    system  3766\n",
       "6978       use  3748\n",
       "5904     agent  3258\n",
       "4120      base  2763\n",
       "...        ...   ...\n",
       "10168     zkdw     1\n",
       "3446      zlwk     1\n",
       "2885   zrunlqj     1\n",
       "5689       zum     1\n",
       "3383       zur     1\n",
       "\n",
       "[10270 rows x 2 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare the results of word frequency on corpus data as a list\n",
    "\n",
    "#freq_word_auto = []\n",
    "freq_word_stem = []\n",
    "\n",
    "# two values or columns in fdist_a\n",
    "for k,v in fdist_stemmed_stop_tokens.items():\n",
    "    freq_word_stem.append([k,v])\n",
    "\n",
    "#make it like an Excel worksheet\n",
    "wordlist_stem = pd.DataFrame(freq_word_stem)\n",
    "\n",
    "#pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "wordlist_stem.sort_values(by=[1,0], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0   1\n",
      "0           xylem   5\n",
      "1       circuitri   1\n",
      "2        orthogon  11\n",
      "3          cussen   1\n",
      "4          yellow   8\n",
      "...           ...  ..\n",
      "10265     approac   1\n",
      "10266   auxiliari  14\n",
      "10267        sasc   9\n",
      "10268  richardson   1\n",
      "10269      invari  27\n",
      "\n",
      "[10270 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(wordlist_stem[:][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "sortwordlist_stem = wordlist_stem.sort_values(by=[1,0], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0     1\n",
      "3134       thi  4448\n",
      "2290    system  3766\n",
      "6978       use  3748\n",
      "5904     agent  3258\n",
      "4120      base  2763\n",
      "...        ...   ...\n",
      "10168     zkdw     1\n",
      "3446      zlwk     1\n",
      "2885   zrunlqj     1\n",
      "5689       zum     1\n",
      "3383       zur     1\n",
      "\n",
      "[10270 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(sortwordlist_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0     1\n",
      "0          thi  4448\n",
      "1       system  3766\n",
      "2          use  3748\n",
      "3        agent  3258\n",
      "4         base  2763\n",
      "...        ...   ...\n",
      "10265     zkdw     1\n",
      "10266     zlwk     1\n",
      "10267  zrunlqj     1\n",
      "10268      zum     1\n",
      "10269      zur     1\n",
      "\n",
      "[10270 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "sortwordlist_stem = sortwordlist_stem.reset_index(drop=True)\n",
    "print(sortwordlist_stem[:][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words =  299345\n",
      "Min number of unique words to account for 15% =  44901.75\n",
      "\n",
      "4448\n",
      "thi\n",
      "3766\n",
      "system\n",
      "3748\n",
      "use\n",
      "3258\n",
      "agent\n",
      "2763\n",
      "base\n",
      "2763\n",
      "data\n",
      "2437\n",
      "inform\n",
      "2349\n",
      "model\n",
      "2249\n",
      "paper\n",
      "1944\n",
      "queri\n",
      "1884\n",
      "user\n",
      "1820\n",
      "learn\n",
      "1597\n",
      "algorithm\n",
      "1575\n",
      "problem\n",
      "1560\n",
      "web\n",
      "1554\n",
      "comput\n",
      "1551\n",
      "applic\n",
      "1546\n",
      "approach\n",
      "1508\n",
      "present\n",
      "1442\n",
      "databas\n",
      "\n",
      "45762\n",
      "20\n",
      "\n",
      "Difference between Sum and the last word qty = 45762 - 1442 = 44320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum_stem = 0\n",
    "j = 0\n",
    "lastwordqty_stem = 0\n",
    "\n",
    "print('Total number of words = ',totwords_stem)\n",
    "print('Min number of unique words to account for 15% = ',totwords15_stem)\n",
    "print()\n",
    "\n",
    "while sum_stem < totwords15_stem:\n",
    "    sum_stem += sortwordlist_stem.loc[j][1]\n",
    "    print(sortwordlist_stem.loc[j][1])\n",
    "    print(sortwordlist_stem.loc[j][0])\n",
    "    lastwordqty_stem = sortwordlist_stem.loc[j][1]\n",
    "    j += 1\n",
    "\n",
    "print()\n",
    "print(sum_stem)\n",
    "print(j)\n",
    "print()\n",
    "\n",
    "diff_stem = sum_stem - lastwordqty_stem\n",
    "print('Difference between Sum and the last word qty =',sum_stem,'-',lastwordqty_stem,'=',diff_stem)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thi 4448\n",
      "system 3766\n",
      "use 3748\n",
      "agent 3258\n",
      "base 2763\n",
      "data 2763\n",
      "inform 2437\n",
      "model 2349\n",
      "paper 2249\n",
      "queri 1944\n",
      "user 1884\n",
      "learn 1820\n",
      "algorithm 1597\n",
      "problem 1575\n",
      "web 1560\n",
      "comput 1554\n",
      "applic 1551\n",
      "approach 1546\n",
      "present 1508\n",
      "databas 1442\n",
      "45762\n",
      "20\n",
      "The minimum number of unique words to account for 15% fo the total number of words in the collection is 20 .\n"
     ]
    }
   ],
   "source": [
    "sum_stem = 0\n",
    "j = 0\n",
    "\n",
    "while sum_stem < totwords15_stem:\n",
    "    sum_stem += sortwordlist_stem.loc[j][1]\n",
    "    print(sortwordlist_stem.loc[j][0], sortwordlist_stem.loc[j][1])\n",
    "    j += 1\n",
    "\n",
    "print(sum_stem)\n",
    "print(j)\n",
    "print('The minimum number of unique words to account for 15% fo the total number of words in the collection is',j,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
